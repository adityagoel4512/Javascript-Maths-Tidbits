<!doctype html>
<html>

<head>
    <!--Page Info-->
    <title>Principal Component Analysis</title>
    <!--css link-->
    <link rel="stylesheet" href="style/skeleton.css">
    <link rel="stylesheet" href="style/style.css">
    <!--Required JS resources--> <!--NB: better to have resources before the main body but still works even if they are not-->
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    <script src=script/numeric/src/numeric.js></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/3.15.0/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pca-js@1.0.0/pca.min.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <script src="./script/jquery-csv.js"></script>
    <script src="https://cdnjs.com/libraries/mathjs"></script>

    <script type="text/javascript">
        divVisibilityMap = new Map([['transformationmaths', false], ['explainedvariancemaths', false], ['covariancematrix', false]]);

        function toggleVisibility(toggleDiv) {
            visible = !divVisibilityMap.get(toggleDiv);
            divVisibilityMap.set(toggleDiv, visible); 
            if (visible) {
                document.getElementById(toggleDiv).style.display = 'block';
            } else {
                document.getElementById(toggleDiv).style.display = 'none';
            }
        }

    </script>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
</head>
<body>

    <div class="container">
            <div id="Dataset" class="tabcontent">
                    <!--html content for here-->
                </div>
                
                <div id="Covariance" class="tabcontent">
                    <!--html content for here-->
                </div>

                <div id="Eigenvectors" class="tabcontent">
                    <!--html content for here-->
                </div>

                <div id="Result" class="tabcontent">
                    <!--html content for here-->
                </div>

            <button class="tablink" onclick="switchTab('Dataset')" id="defaultOpen">Analyse Dataset</button>
            <button class="tablink" onclick="switchTab('Covariance')">Compute Covariance Matrix</button>
            <button class="tablink" onclick="switchTab('Eigenvectors')">Compute Eigenvectors and Dimensionality Reduction</button>
            <button class="tablink" onclick="switchTab('Result')">Result and Data Reconstruction</button>
        
        <div class="row">
            
            <!-- Graph -->
     
            <div class="six columns">
                <div id='padding' style="widows: 400%; height:20px;"></div>
                <h1>Principal Component Analysis</h1>
                <div class='covariance_matrix' style="widows: 600%px; height:550%px; display:none">
                    <body> \(cov (A, B) = E[(A-E[A])] - E[(B-E[B])]\)<br /><br />
                            The  Covariance  Matrix for random variables W, X, Y, Z:
                            $$
                                \begin{bmatrix}
                                cov (W, W) & cov (W, X) & cov (W, Y) & cov (W, Z) \\
                                cov (X, W) & cov (X, X) & cov (X, Y) & cov (X, Z) \\
                                cov (Y, W) & cov (Y, X) & cov (Y, Y) & cov (Y, Z) \\
                                cov (Z, W) & cov (Z, X) & cov (Z, Y) & cov (Z, Z) \\
                                \end{bmatrix}
                            $$  
                            <br />
                            Going back to our example data, the random variables are \(sepal width, sepal length, petal width, petal length\). You can compute all the covariances pairwise to the right. The covariance matrix is then populated as shown above.
                            <br /><br />
                            <button class="covariance_matrix" onclick="toggleVisibility('covariancematrix');">More on spread of data</button>
                    </body>       
                 </div>

                 <div id='covariancematrix' style="widows: 600%px; height:550%px; display:none">
                    
                        <body>
                            We can quantify how 'spread' data is by looking at a dataset's standard deviation or variance. If we think about how spread a dataset is, 
                            it makes sense to look at the mean and see how much all of the datapoints deviate from this. This is what the The Fisher's Iris Dataset we've been investigating 
                            is a sample from a population obviously.

                            <br/><br/>

                            If the kth principal component P<sub>k</sub> has an associated eigenvalue λ<sub>k</sub>, and we have n principal components, then the Explained Variance of P<sub>k</sub> is
                            
                            $$
                                \frac{λ_k}{\sum_{i=0}^n λ_i}
                            $$

                            i.e. calculating PC1's explained variance as above is done as follows:
                            
                            $$
                                \frac{2.91...}{2.91... + 0.147... + 0.921... + 0.021...} = 0.730...
                            $$
                        </body>
                </div>
                
                 <div class='variance_maths' style="widows: 600%px; height:550%px; display:none">
                    <body>  <br /><br />To understand standard deviation, we need a data set. Statisticians are usually concerned with taking a sample of a population. To use election polls as an example, the
                            population is all the people in the country, whereas a sample is a subset of the population that the statisticians measure. The great thing about statistics is that by only
                            measuring (in this case by doing going out and measuring some flowers) a sample of the population, you can work out what is most likely to be the measurement if you used the entire population. 
                            From here on out, we assume all data is sampled. It's worth reading up on sampling if you have the time.<br><br> One of the things we can compute from our dataset is mean, as follows: 
                            $$\bar X = \frac{\sum_{i=0}^n X_i}{n}$$                    
                            Another feature of the data we may wish to understand is the spread. Highly spread data, as you can see, has a high magnitude standard deviation and thus a large variance - this should be obvious below.
                            $$sample \: standard \: deviation = \sigma = \sqrt {\frac{\sum_{i=0}^n (X_i - \bar X)^2}{n - 1}}$$                    
                            $$sample \: variance = s^2$$                   
                            <br/><br/>

                             
                    </body>       
                 </div>
                <div id='graph1' style="widows: 600%px; height:550%px"></div>
                <div class='transformation3' style="widows: 600%px; height:550%px; display:none">
                    <body>
                            $$
                                Reduced Data = Normalised Original Data \cdot Projection Matrix
                            $$

                            $$
                                = 
                                \begin{bmatrix}
                                -0.90 & 1.03 & -1.34 & -1.31 \\
                                -1.14 & -0.12 & -1.33 & -1.30 \\
                                -1.38 & 0.34 & -1.40 & -1.31 \\
                                \vdots & \vdots & \vdots & \vdots \\
                                \end{bmatrix}
                                \cdot
                                \begin{bmatrix}
                                -0.52 & -0.37 & 0.72 \\
                                0.26 & -0.93 & -0.24 \\
                                -0.57 & -0.07 & -0.63 \\
                                -0.52 & -0.37 & 0.72 \\
                                \end{bmatrix}
                            $$
                    </body>
                    <br />
                    <button class="transformation_maths" onclick="toggleVisibility('transformationmaths');">See some of the maths behind the transformation</button>
                </div>
                <div class='transformation2' style="widows: 600%px; height:550%px; display:none">
                    <body>
                            $$
                                Reduced Data = Normalised Original Data \cdot Projection Matrix
                            $$

                            $$
                                = 
                                \begin{bmatrix}
                                -0.90 & 1.03 & -1.34 & -1.31 \\
                                -1.14 & -0.12 & -1.33 & -1.30 \\
                                -1.38 & 0.34 & -1.40 & -1.31 \\
                                \vdots & \vdots & \vdots & \vdots \\
                                \end{bmatrix}
                                \cdot
                                \begin{bmatrix}
                                -0.52 & -0.37 \\
                                0.26 & -0.93 \\
                                -0.57 & -0.07 \\
                                -0.52 & -0.37 \\
                                \end{bmatrix}
                            $$
                    </body>
                    <br /> 
                    <button class="transformation_maths" onclick="toggleVisibility('transformationmaths');">See some of the maths behind the transformation</button>
                </div>
            <div id='transformationmaths' style="widows: 600%px; height:550%px; display:none">
                <body>
                    Intuitively speaking, an eigenvector is a vector that remains unchanged when a linear transformation is applied to it.<br /><br />
                    i.e Let C be our square covariance matrix, ν a vector and λ a scalar that satisfies Cν = λν, then λ is an eigenvalue associated with eigenvector ν of A.<br /><br />
                    We can therefore compute the eigenvalues by solving the characteristic equation below for λ:
                    
                    $$
                    det(C - λI_n) = 0
                    $$

                    With each eigenvalue λ we can then compute its eigenvector(s) computing ν by gaussian elimination.

                    $$
                    (C - λ)\cdotν = 0
                    $$
    
                    We started with the goal to reduce the dimensionality of our feature space, 
                    i.e., projecting the feature space via PCA onto a smaller subspace, where the eigenvectors will form the axes of this new feature subspace.
                    In order to decide which eigenvectors we want to drop for our lower-dimensional subspace, we have to take a look at the corresponding eigenvalues of the eigenvectors.
                    
                    <br/><br/>

                    The eigenvectors with the lowest eigenvalues bear the least information about the distribution of data, so we rank eigenvectors by their corresponding
                    eigenvalues.
                    
                    <br/><br/>

                    Ordered Eigenvectors from the Fisher's Iris Dataset:
                    $$
                    \begin{bmatrix}
                    -0.52 \\
                    0.26 \\
                    -0.57 \\
                    -0.52 \\
                    \end{bmatrix},
                    \begin{bmatrix}
                    -0.37 \\
                    -0.93 \\
                    -0.07 \\
                    -0.37 \\
                    \end{bmatrix},
                    \begin{bmatrix}
                    0.72 \\
                    -0.24 \\
                    -0.63 \\
                    -0.72 \\
                    \end{bmatrix},...
                    $$

                    We then choose the transpose of the augmented matrix formed of the top k eigenvectors to be our transformation matrix W that transforms our normalised original data onto the k dimensional subspace.
                    So for when k = 2, 

                    $$
                        W_2^T =
                        \begin{bmatrix}
                        -0.52 & -0.37 \\
                        0.26 & -0.93 \\
                        -0.57 & -0.07 \\
                        -0.52 & -0.37 \\
                        \end{bmatrix}
                    $$ 
                    
                    So if our normalised data is represented as matrix X, the plot we see above (when the number of principal components is 2) is the computed as follows: 

                    $$
                        W_2 \cdot X
                    $$

                </body>
            </div>
            <br /><br />

            </div>

            <!--Bottons/Sliders-->
            <div class="six columns">
                    
                <!--Main Options/Tabs-->
                <!--BEGIN Slider-->
                    <!--These Sliders are shared among all tabs-->

                    <!--Slider Display--> <!--Linked with the slider below-->
                    <!--Linked with a slider and shows the live values of its corresponding slider-->
                    <div id='information' style="widows: 600%; height:320%px;"></div>
                    <div id='params' style="widows: 600%; height:100%px;"></div>
                    <div id='graph2' style="widows: 600%;; height:550%px;"></div>
                    <button class="explained_variance_maths" style="display:none;" onclick="toggleVisibility('explainedvariancemaths')">See some of the maths behind the 'Explained Variance'</button>
                    <div id='explainedvariancemaths' style="widows: 600%px; height:550%px; display:none">
                            <body>
                                Explained variance is a term we use to describe the proportion of the total variation in the data accounted for by a principal component.
                                The eigenvalues themselves encode this as they come from the eigendecomposition of the covariance matrix.
                                <br/><br/>

                                If the kth principal component P<sub>k</sub> has an associated eigenvalue λ<sub>k</sub>, and we have n principal components, then the Explained Variance of P<sub>k</sub> is
                                
                                $$
                                    \frac{λ_k}{\sum_{i=0}^n λ_i}
                                $$

                                i.e. calculating PC1's explained variance as above is done as follows:
                                
                                $$
                                    \frac{2.91...}{2.91... + 0.147... + 0.921... + 0.021...} = 0.730...
                                $$
                            </body>
                    </div>
                    <br /><br />
            </div>
        </div>
    </div>

    <!--THE BRAINS-->
    <script src=script/objects.js></script>
    <script src=script/pca.js></script>
</body>

</html>
